{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransferLearning_Tutorial.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa6I7ZDTxS3s",
        "colab_type": "text"
      },
      "source": [
        "# Using pre-trained models for Texture Classification\n",
        "In this lab, we will use pre-trained models for the same texture classification task. The dataset we will be using is available here: http://dx.doi.org/10.5281/zenodo.53169. \n",
        "\n",
        "![alt text](https://www.researchgate.net/profile/Jakob_Kather/publication/303998214/figure/fig7/AS:391073710002224@1470250646407/Representative-images-from-our-dataset-Here-the-first-10-images-of-every-tissue-class.png)\n",
        "\n",
        "The above figure shows the 8 different classes of tissue we will be trying to identify. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgBlcKdzGR9v",
        "colab_type": "text"
      },
      "source": [
        "# Imports\n",
        "\n",
        "Importing important modules, including tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG_by_wHxS3w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Basic operating system (os), numerical, and plotting functionality\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from matplotlib import pylab as plt\n",
        "\n",
        "# scikit-learn data utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage import transform\n",
        "\n",
        "# Color transformations\n",
        "from skimage.color import rgb2lab\n",
        "\n",
        "# scikit-learn performance metric utilities\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Import our neural network building tools\n",
        "import tensorflow as tf\n",
        "\n",
        "# Garbage collection (for saving RAM during training)\n",
        "import gc"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bue-CFYq2WAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set plotting preferences\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "font = {'family' : 'sans-serif',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 16}\n",
        "matplotlib.rc('font', **font)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baMNmzabxS4A",
        "colab_type": "text"
      },
      "source": [
        "# InceptionV3 Model\n",
        "\n",
        "In this notebook we will use the InceptionV3 model as the feature extractor. By downloading the model and its pretrained weights, we can leverage it for our own image classification problem. The model has been trained on the ImageNet image classification dataset. (For more details on this dataset see http://www.image-net.org/.)\n",
        "\n",
        "<br>\n",
        "\n",
        "Let's examine the InceptionV3 model. The model has the following specifications that are relevant for our task:\n",
        "* Expected input size : 299x299 pixels\n",
        "* Number of output classes : 1000\n",
        "\n",
        "Our images are 150x150 pixels in size and come from only eight categories. In order to use this model for our classification task, we need to do the following:\n",
        "* Resize images : Our input images can be resized to the appropriate dimensions. Alternatively, we can pad our images to the expected dimensions. Padding leads to additional choices - Do we pad with zeros, duplicate edge pixels or mirror the image across edges ?\n",
        "* Change the prediction layer : Remove the existing prediction layer and add a new layer that can predict 8 classes.\n",
        "* Train : Finally, we need to train the network on our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xecY-WtLxS4C",
        "colab_type": "text"
      },
      "source": [
        "# Retrieve and Load the Data\n",
        "\n",
        "The following cells download the data from public Medlytics data repository and load those images and labels into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyrK2wvrWEZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "7883d91f-99c7-4330-97b5-9263831748e4"
      },
      "source": [
        "# Define the current directory and the directory where the files to download can\n",
        "# be found\n",
        "current_dir = os.getcwd()\n",
        "remote_path = 'https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/'\n",
        "\n",
        "# Define and build a directory to save this data in\n",
        "data_dir = os.path.join(current_dir, 'crc_data')\n",
        "if not os.path.isdir(data_dir):\n",
        "  os.mkdir(data_dir)\n",
        "\n",
        "# Move into the data directory and download all of the files\n",
        "os.chdir(data_dir)\n",
        "for ii in range(1, 6):\n",
        "    basename = f'rgb0{ii}.npz'\n",
        "    filename = os.path.join(remote_path, basename)\n",
        "\n",
        "    # Check if the file has already been downloaded\n",
        "    if not os.path.isfile(basename):\n",
        "      cmd = f'wget {filename}'\n",
        "      print(cmd)\n",
        "      os.system(cmd)\n",
        "\n",
        "# Return to the original directory\n",
        "os.chdir(current_dir)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wget https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/rgb01.npz\n",
            "wget https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/rgb02.npz\n",
            "wget https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/rgb03.npz\n",
            "wget https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/rgb04.npz\n",
            "wget https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/NotebookExampleData/Week3/data_nuclei/crc/rgb05.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6bIoXFUWuxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a function to load the data from the assumed download path\n",
        "def load_images(colorspace='rgb'):\n",
        "    \"\"\"\n",
        "    Loads the example data and applies transformation into requested colorspace\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    colorspace : str, optional, default: `rgb`\n",
        "        The colorspace into which the images should be transformed. Accepted\n",
        "        values include\n",
        "\n",
        "        'rgb' : Standard red-green-blue color-space for digital images\n",
        "\n",
        "        'gray' or 'grey': An arithmetic average of the (r, g, b) values\n",
        "\n",
        "        'lab': The CIE L*a*b* colorspace\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    images : numpy.ndarray, shape (Nimg, Ny, Nx, Ncolor)\n",
        "        The complete set of transformed images\n",
        "\n",
        "    labels : numpy.ndarray, shape (Nimg)\n",
        "        The classification labels associated with each entry in `images`\n",
        "\n",
        "    label_to_str : dict\n",
        "        A dictionary which converts the numerical classification value in\n",
        "        `labels` into its string equivalent representation.\n",
        "    \"\"\"\n",
        "    # Check that the colorspace argument is recognized\n",
        "    colorspace_lower = colorspace.lower()\n",
        "    if colorspace_lower not in ['rgb', 'gray', 'grey', 'lab']:\n",
        "        raise ValueError(f'`colorspace` value of {colorspace} not recognized')\n",
        "\n",
        "    # Load data, which is stored as a numpy archive file (.npz)\n",
        "    filename = os.path.join(data_dir, 'rgb01.npz')\n",
        "    print(f'loading {filename}')\n",
        "    tmp = np.load(os.path.join(data_dir, 'rgb01.npz'), allow_pickle=True)\n",
        "\n",
        "    # Parse the loaded data into images and labels\n",
        "    # Initialize the images and labels variables using the first archive data\n",
        "    images = tmp['rgb_data']\n",
        "    if colorspace_lower == 'rgb':\n",
        "        pass\n",
        "    elif colorspace_lower in ['gray', 'grey']:\n",
        "        images = np.mean(images, axis=-1)      # Average into grayscale\n",
        "    elif colorspace_lower == 'lab':\n",
        "        images = rgb2lab(images)               # Convert to CIE L*a*b*\n",
        "\n",
        "    # Grab the initial array for the image labels\n",
        "    labels = tmp['labels']\n",
        "    \n",
        "    # Grab the dictionary to convert numerical labels to their string equivalent\n",
        "    label_to_str = tmp['label_str']\n",
        "    label_to_str = label_to_str.tolist() # Convert label_to_str into a dict\n",
        "\n",
        "    # Update the user on the number and size of images loaded\n",
        "    print('Loaded images with shape {}'.format(images.shape))\n",
        "    del tmp\n",
        "\n",
        "    # Loop over each of the remaining archives and append the contained data\n",
        "    for ii in range(2,6):\n",
        "        # Build the full path to the archive and load it into memory\n",
        "        filename = os.path.join(data_dir, f'rgb0{ii}.npz')\n",
        "        print(f'loading {filename}')\n",
        "        tmp = np.load(filename, allow_pickle=True)\n",
        "\n",
        "        # Parse and append the data\n",
        "        these_images = tmp['rgb_data']\n",
        "        if colorspace_lower == 'rgb':\n",
        "            pass\n",
        "        elif (colorspace_lower == 'gray') or (colorspace_lower == 'grey'):\n",
        "            these_images = np.mean(these_images, axis=-1) # Convert to grayscale\n",
        "        elif colorspace_lower == 'lab':\n",
        "            these_images = rgb2lab(these_images)          # Convert to CIEL*a*b*\n",
        "\n",
        "        # Append the images and labels\n",
        "        images = np.append(images, these_images, axis=0)\n",
        "        labels = np.append(labels, tmp['labels'], axis=0)\n",
        "\n",
        "        # Update the user on the number and size of images\n",
        "        print('Loaded images with shape {}'.format(these_images.shape))\n",
        "        del tmp\n",
        "\n",
        "    # Force the image data to be floating point and print the data shape\n",
        "    images = images.astype(np.float)\n",
        "    print('Final image data shape: {}'.format(images.shape))\n",
        "    print('Number of image labels: {}'.format(*labels.shape))\n",
        "\n",
        "    return images, labels, label_to_str"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwuZxiILWxDr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "e20d5cee-1a07-4dd4-c202-44defeae7797"
      },
      "source": [
        "# Load images and labels into memory and count the number of classes\n",
        "images_full_res, labels, label_to_str = load_images()\n",
        "num_classes = np.unique(labels).size"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading /content/crc_data/rgb01.npz\n",
            "Loaded images with shape (1000, 150, 150, 3)\n",
            "loading /content/crc_data/rgb02.npz\n",
            "Loaded images with shape (1000, 150, 150, 3)\n",
            "loading /content/crc_data/rgb03.npz\n",
            "Loaded images with shape (1000, 150, 150, 3)\n",
            "loading /content/crc_data/rgb04.npz\n",
            "Loaded images with shape (1000, 150, 150, 3)\n",
            "loading /content/crc_data/rgb05.npz\n",
            "Loaded images with shape (1000, 150, 150, 3)\n",
            "Final image data shape: (5000, 150, 150, 3)\n",
            "Number of image labels: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx3V9darDHps",
        "colab_type": "text"
      },
      "source": [
        "# Pre-process the Images\n",
        "\n",
        "Apply the usual pre-processing steps to this image data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HFa8WZcEBRt",
        "colab_type": "text"
      },
      "source": [
        "## Resize the Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SmzcDTx0gtG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "2f575d67-b839-4523-aeac-b70c162c7e30"
      },
      "source": [
        "# This boolean can be switched to false if you do not want to resize the images\n",
        "resize_images_bool = True\n",
        "\n",
        "# Specify a new shape to use for the resized images\n",
        "# NOTE: For the InceptionV3 model, we must use a size of at least (75, 75).\n",
        "# Other models may allow smaller input images.\n",
        "original_shape = images_full_res.shape\n",
        "new_shape = list(original_shape)\n",
        "new_shape[1:3] = (299, 299) # put any image size you want!\n",
        "\n",
        "# Compute if we are downsampling (in which case we need anti-aliasing)\n",
        "scaling_ratio = np.array(new_shape[1:3])/np.array(original_shape[1:3])\n",
        "anti_alias = np.any(scaling_ratio < 1)\n",
        "\n",
        "# If resizing is requested, then run the resizing transformation\n",
        "if resize_images_bool:\n",
        "    # Grab the original shape of the images\n",
        "    num_images = images_full_res.shape[0]\n",
        "\n",
        "    # Initialize an array for storing the resized images\n",
        "    images = np.zeros(new_shape, dtype=np.float16)\n",
        "\n",
        "    # Loop over each image in the data and perform a resizing operation\n",
        "    for img_num, img_data in enumerate(images_full_res):\n",
        "        # Update the user on progress\n",
        "        if np.mod(img_num, 1000) == 0:\n",
        "            print(f'Processing image number {img_num}')\n",
        "\n",
        "        # Process the image and force it to be a 16-bit float\n",
        "        processed_img = transform.resize(img_data, new_shape[1:],\n",
        "                                         anti_aliasing=anti_alias)\n",
        "        images[img_num] = processed_img.astype(np.float16)\n",
        "\n",
        "# If no resizing requested, then just rename that data\n",
        "else:\n",
        "    images = images_full_res\n",
        "\n",
        "# Remove the full-resolution versions from memory (just clogging things up)\n",
        "del images_full_res\n",
        "\n",
        "print('Done!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing image number 0\n",
            "Processing image number 1000\n",
            "Processing image number 2000\n",
            "Processing image number 3000\n",
            "Processing image number 4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB5x4euLEGmv",
        "colab_type": "text"
      },
      "source": [
        "## Normalize the Images\n",
        "\n",
        "The model we will be adpopting uses batch normalization, which suggests that we should scale the images to the range $\\left[-1, +1\\right]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpwV7XaMxS4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scale image values to the proper data range (if it hasn't been done already)\n",
        "# Note: Think *very* carefully about what normalizaiton needs to be applied. In\n",
        "# this example we originally constructed, the image data was in the range\n",
        "# [0, 255], but you may try some different datasets and need to modify this\n",
        "# normalization cell.\n",
        "# images = images.astype(np.float16)/127.5 - 1\n",
        "images = images.astype(np.float16)/255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5rNnXfmQwRB",
        "colab_type": "text"
      },
      "source": [
        "## Include an Axis for Color Channels\n",
        "In the case where we are operating on grayscale data, there should be an axis at the end of the array of length 1 to match up with TensorFlow expectations. Otherwise, you may get an error about not having the right number of dimensions for input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_v912cBGrR4D",
        "colab": {}
      },
      "source": [
        "# Take note of number of color channels in the loaded image add a last axis to \n",
        "# images ndarray if array dimension is only 3 (as is the case with grayscale images)\n",
        "if images.ndim == 3:\n",
        "    # If image is grayscale, then we add a last axis (of len 1) for channel\n",
        "    n_channels = 1\n",
        "    images = images[:, : , :, np.newaxis]\n",
        "    print('\\nlast dimension added to images ndarray to account for channel')\n",
        "    print(f'new images.shape: {images.shape}')\n",
        "else:\n",
        "    #if image is not grayscale, last dimension of image already corresponds to channel\n",
        "    n_channels = images.shape[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXl0lLn0EX0L",
        "colab_type": "text"
      },
      "source": [
        "## Split the Image and Label Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRjyL6MixS4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split data into train and test sets\n",
        "tmp = train_test_split(images, labels, test_size=.2)\n",
        "train_images, test_images, train_labels, test_labels = tmp\n",
        "\n",
        "# Convert 'labels' (1D array of integers) to one-hot encoding\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "# Print sizes of train/test sets\n",
        "print(f'train_images.shape: {train_images.shape}')\n",
        "print(f'train_labels.shape: {train_labels.shape}')\n",
        "print(f'test_images.shape: {test_images.shape}')\n",
        "print(f'test_labels.shape: {test_labels.shape}')\n",
        "\n",
        "# Print the one-hot encoded labels as a sanity check\n",
        "print('one-hot encoded labels:')\n",
        "print(train_labels)\n",
        "\n",
        "# Get rid of the duplicate copies of the data\n",
        "del images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6eudXTcxS39",
        "colab_type": "text"
      },
      "source": [
        "# Use Transfer Learning to Build the CNN\n",
        "\n",
        "Load a pre-trained version of the InceptionV3 model. The weights in this model were learned from the (*very* large ImageNet dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYVrwwTIRpTc",
        "colab_type": "text"
      },
      "source": [
        "## Load the Pre-Trained CNN Architecture and Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG1xVLryxS4h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Loading Pre-Trained Model')\n",
        "# Load the base, pre-trained model\n",
        "#\n",
        "# -- Arguments --\n",
        "# input_shape : simply grab the (nrows, ncols, nchannels) part of the image data\n",
        "# include_top : do *not* download the final, classification layers of the model\n",
        "# weights : use the weights learned from the ImageNet dataset\n",
        "#\n",
        "base_model = tf.keras.applications.InceptionV3(input_shape=train_images.shape[1:],\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQyzcgCfSnhf",
        "colab_type": "text"
      },
      "source": [
        "Let's go ahead and take a look at the architecture of the base model... but be warned... it is *very* deep.\n",
        "\n",
        "<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/raw/master/Images/Week3/leo_deeper_cnn.jpg\" width=\"500\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsiXfJBxSs1o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the model summary to screen\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFq32f86xS4e",
        "colab_type": "text"
      },
      "source": [
        "## Freeze the Convolutional Base\n",
        "\n",
        "The following cell forces the convolutional part of the CNN to remain \"frozen\" so that none of the filter weights are changed during the training process. This approach assumes that the weights learned during the pre-training on the ImageNet dataset are sufficiently useful for our new application, so we will leave them as they are. In practice, it seems that freezing layers increase the training speed while have little impact on accuracy.\n",
        "\n",
        "It is also possible to freeze specific layers within the CNN by setting `layer.trainable = False`, but since the InceptionV3 model has so many layers, we will simply use a single command to freeze *all* of the layers in the base model. You can read more about Freezing Layers here: https://keras.io/guides/transfer_learning/#freezing-layers-understanding-the-trainable-attribute.\n",
        "\n",
        "Why would we need these layers in the first place? Well, sometimes we need a very deep NN (such as this pretrained model with its useful weights), but we don't have enough time or data to train it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re8TFpMPSeHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze the entire base model\n",
        "base_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0EvBvCZ9x17",
        "colab_type": "text"
      },
      "source": [
        "In the following cell, we define some functions that may be useful for experimenting with unfreezing specific layers of the model. In general, with a smaller dataset, it is not advised that you make *all* layers trainable. Instead, you should leave at least some of the layers in the pre-trained model frozen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O7oQxX_86oZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define some functions for experimenting with unfreezing layers\n",
        "def unfreeze_layers(model, top_n_layers):\n",
        "    \"\"\"Makes the top layers of the `base_model` trainable (except batch norm)\"\"\"\n",
        "    # We unfreeze the top layers while leaving BatchNorm layers frozen\n",
        "    for layer in model.layers[-top_n_layers:]:\n",
        "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "            layer.trainable = True\n",
        "\n",
        "def freeze_layers(model, top_n_layers):\n",
        "    \"\"\"Makes the top layers of the `base_model` untrainable\"\"\"\n",
        "    # We unfreeze the top layers while leaving BatchNorm layers frozen\n",
        "    for layer in model.layers[-top_n_layers:]:\n",
        "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
        "            layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7SbyEYx9O0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### OPTIONAL: Experiment with unfreezing some layers in the pretrained model\n",
        "# n_layers = 20\n",
        "# unfreeze_layers(model, n_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Y7VogQTA5L",
        "colab_type": "text"
      },
      "source": [
        "## Add a Global Average Pooling Layer\n",
        "\n",
        "By adding a global pooling layer, we can guarantee that the output from the base model will be translated into a single 2048 element vector for each input image, even if the size of the input image is increased! This is convenient because it allows us to play with different image sizes without needing to re-design our final CNN model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J_bGUTmxS4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a global pooling layer to be added into the model\n",
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbHzoezMXJiG",
        "colab_type": "text"
      },
      "source": [
        "## Build a Prediction Layer\n",
        "\n",
        "As a final step in the transfer learning process, we need to add a layer which will convert those 2048 element feature vectors into predicted classifications. This can be accomplished by a single dense layer with an output size equal to the number of classes (which we have conveniently stored in a variable named `num_classes`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsl7Q_JKUXGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's add a fully-connected layer\n",
        "prediction_layer1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "\n",
        "# Add the final classification layer\n",
        "prediction_layer2 = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "# # Build a single fully-connected layer to perform the final classifications\n",
        "# prediction_layer = tf.keras.layers.Dense(num_classes, activation='softmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AEUNafkU9d1",
        "colab_type": "text"
      },
      "source": [
        "## Combine Components into Final Model\n",
        "\n",
        "Now, we can combine the feature-extracting \"base_model,\" the global pooling average layer, and the fully-connected classification layer to produce a single, final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPnFV2XlU88_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the final model as a sequence:\n",
        "# input --> feature extractor --> global pooling --> classifier --> predictions\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    global_average_layer,\n",
        "    prediction_layer1,\n",
        "    prediction_layer2\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2eROXG4vF_u",
        "colab_type": "text"
      },
      "source": [
        "## Compile and Train the Model\n",
        "\n",
        "Specify the loss function and training algorithm to be used, then compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op8lCpk8YfvF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify the loss function to use\n",
        "loss_func = tf.keras.losses.categorical_crossentropy\n",
        "\n",
        "# Use the RMSprop learning algorithm to optimize the network weights\n",
        "base_learning_rate = 0.0001\n",
        "opt = tf.keras.optimizers.RMSprop(lr=base_learning_rate)\n",
        "\n",
        "# Compile the model using the specified loss function and potimizer\n",
        "model.compile(loss=loss_func, optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBWN5CYKZJ56",
        "colab_type": "text"
      },
      "source": [
        "Train the model on the new, histological, data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOe9aVfvyaky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all of our training and validation ('test') data to TensorFlow data\n",
        "# This prevents the training algorithm from needing to make a *copy* of your\n",
        "# numpy arrays, which would EAT UP SOO MUCH RAM!\n",
        "#\n",
        "# It also accelerates training a bit because there is no data-conversion step\n",
        "train_images_tf = tf.constant(train_images, dtype=tf.float16)\n",
        "test_images_tf = tf.constant(test_images, dtype=tf.float16)\n",
        "del train_images, test_images\n",
        "\n",
        "train_labels_tf = tf.constant(train_labels)\n",
        "test_labels_tf = tf.constant(test_labels)\n",
        "del train_labels, test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nIs4YHSxS4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function is called after each epoch\n",
        "# (It will ensure that your training process does not consume all available RAM)\n",
        "class garbage_collect_callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    gc.collect()\n",
        "\n",
        "# Time how long it takes the model to train for these epochs\n",
        "start_time = time.time()\n",
        "\n",
        "# Perform the training method\n",
        "history = model.fit(train_images_tf,\n",
        "                    train_labels_tf,\n",
        "                    batch_size=32,\n",
        "                    epochs= 12,\n",
        "                    verbose=True,\n",
        "                    validation_data=(test_images_tf, test_labels_tf),\n",
        "                    callbacks = [garbage_collect_callback()])\n",
        "\n",
        "stop_time = time.time()\n",
        "print(\"--- %s seconds ---\" % (stop_time - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWHtc539dJLl",
        "colab_type": "text"
      },
      "source": [
        "### Example Results for Different Input Image Sizes\n",
        "\n",
        "After running some experiment swith different input image sizes, we found the following results.\n",
        "\n",
        "\n",
        "| Resizing    | Validation Accuracy  |\n",
        "| ----------- | -------------------- |\n",
        "|  75 pixels  | $78.7\\%$             |\n",
        "| 150 pixels  | $84.7\\%$             |\n",
        "| 250 pixels  | $88.2\\%$             |\n",
        "| 299 pixels  | $90.4\\%$             |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPUn3FKwqnXU",
        "colab_type": "text"
      },
      "source": [
        "## Examine the Training History\n",
        "\n",
        "Let's plot the progress of the training procedure to see if we started to overfit at any point. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSssjwtuHCs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot model train/validation accuracy and model train/validation loss\n",
        "# Summarize history for accuracy\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Summarize history for loss\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9i1lH0fxS4y",
        "colab_type": "text"
      },
      "source": [
        "# Predict Classes for the Test Images\n",
        "Use trained model to predict classes of test and then plot some example predicted classifications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poXgbWKGxS4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict class of test each test\n",
        "predictions = model.predict(test_images_tf, verbose=True)\n",
        "\n",
        "# Convert the predictions and true labels into category numbers\n",
        "test_true_labels = test_labels_tf.numpy().argmax(axis=1)\n",
        "test_pred_labels = predictions.argmax(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9meUM-e2xS44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot a set of test images, along with predicted labels and true labels\n",
        "plt.figure(figsize=(16,20))\n",
        "for ii in range(0, 16):\n",
        "    # Activate subplot and display image\n",
        "    plt.subplot(4,4,ii+1)\n",
        "    plt.imshow(test_images_tf[ii+100,:,:,:].numpy().astype(np.float))\n",
        "\n",
        "    # Turn off axes\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Add annotaiton\n",
        "    plt.title('expected : ' + label_to_str[test_true_labels[ii+100]]\n",
        "              + '\\npredicted : ' + label_to_str[test_pred_labels[ii+100]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdoq1Epj6eMZ",
        "colab_type": "text"
      },
      "source": [
        "To conclude this section. Let's Look at the final computed accuracy and confusion matrix for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZfvOSrJ7TYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compute the model accuracy and print it to the user\n",
        "acc = accuracy_score(test_true_labels, test_pred_labels)\n",
        "print(f'Model Accuracy: {acc:.2%}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGIr62h668wR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate the confusion matrix using these labels\n",
        "conf_mat = confusion_matrix(test_true_labels, test_pred_labels)\n",
        "\n",
        "# Generate a new figure\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "# Display the confusion matrix\n",
        "plt.imshow(conf_mat, cmap='hot', interpolation='nearest')\n",
        "\n",
        "# Add some anotation for the plot\n",
        "plt.colorbar()\n",
        "plt.xlabel('True label')\n",
        "plt.ylabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmXCQWDUm6VJ",
        "colab_type": "text"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "* The purpose of transfer learning is to adapt existing trained neural networks to different classification problems\n",
        "* Transfer learning allows us to train a neural network classifier much faster and with less data\n",
        "* By 'freezing' layers of the existing trained network, we can ensure training only takes place within a smaller subset of network layers -- this ability affords us the benefits of a large well-designed expressive network without requiring creation from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGE4lmGh6llq",
        "colab_type": "text"
      },
      "source": [
        "## Additional Experiments to Run\n",
        "\n",
        "* Try un-freezing some or all of the \"base_model\" layers.\n",
        "\n",
        "* Try comparing te un-frozen version of the model across a variety of input image sizes."
      ]
    }
  ]
}